{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK предоставляет доступ к множеству текстовых корпусов и предварительно обученных моделей,  \n",
    "которые могут быть полезны в различных задачах NLP. Эти данные не устанавливаются автоматически с библиотекой,   \n",
    "поэтому их нужно загрузить отдельно. Для этого используйте следующий код:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружает наиболее часто используемые корпуса и модели\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизация — это процесс разбиения текста на более мелкие части, такие как слова или предложения. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Этот код загрузит необходимые данные punkt, которые используются для токенизации текста.\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбиение на слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLTK', 'упрощает', 'обработку', 'текста', '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"NLTK упрощает обработку текста.\"\n",
    "word_tokens = word_tokenize(text)\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбиение на предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Одно предложение.', 'Другое предложение.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Одно предложение. Другое предложение.\"\n",
    "sentence_tokens = sent_tokenize(text)\n",
    "sentence_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удаление стоп-слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стоп-слова — это общеупотребительные слова в языке, которые обычно несут мало смысловой нагрузки (например, \"и\", \"в\", \"на\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLTK', 'помогает', 'удалении', 'стоп-слов', 'текста', '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"NLTK помогает в удалении стоп-слов из текста.\"\n",
    "tokens = word_tokenize(text)\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удаление знаков пунктуации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLTK', 'помогает', 'удалении', 'стоп-слов', 'текста']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation = set(string.punctuation)\n",
    "filtered_tokens = [token for token in filtered_tokens if token not in punctuation]\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стемминг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стемминг — это процесс сведения слов к их основной (корневой) форме, удаляя окончания и суффиксы.   \n",
    "Это помогает уменьшить сложность текста и улучшить производительность алгоритмов анализа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разница между стемминг (stemming) и лемматизацией заключается в том, что лемматизация учитывает контекст  \n",
    "и преобразует слово в его значимую базовую форму, тогда как стемминг просто удаляет последние несколько символов,   \n",
    "что часто приводит к неверному значению и орфографическим ошибкам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стемминг на английском языке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'stem', 'form', 'of', 'leav', 'is', 'leaf']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "text = \"The stemmed form of leaves is leaf\"\n",
    "tokens = word_tokenize(text)\n",
    "stemmed_words = [stemmer.stem(token) for token in tokens]\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стемминг на русском языке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['листов', 'зашел', 'листочк', 'лист', 'листв', 'листв', 'поч', 'так']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "text = \"Листовые листочки лист листва листве почему так\"\n",
    "tokens = word_tokenize(text)\n",
    "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В отличие от стемминга, лемматизация сводит слова к их лемме — это более сложный процесс, который учитывает морфологический анализ слов.  \n",
    "Лемматизация более точно обрабатывает слова, приводя их к словарной форме."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизация на английском языке:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно загрузить данные omw-1.4 с помощью NLTK Downloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'lemmatized', 'form', 'of', 'leaf', 'is', 'leaf']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "text = \"The lemmatized form of leaves is leaf\"\n",
    "tokens = word_tokenize(text)\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизация на русском языке   \n",
    "используя стеммер, так как для русского языка NLTK не предоставляет прямой лемматизатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['лемматизирова', 'форм', 'слов', 'лист', 'эт', 'лист']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"russian\")\n",
    "text = \"Лемматизированная форма слова листья это лист\"\n",
    "tokens = word_tokenize(text)\n",
    "lemmatized_words = [stemmer.stem(word) for word in tokens]\n",
    "lemmatized_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Анализ настроений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Анализ настроений, иногда называемый \"определением тональности\", включает использование NLP,   \n",
    "статистических или машинно-обученных алгоритмов для изучения, идентификации и извлечения информации о настроениях из текстов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Анализ настроений (или сентимент-анализ) в NLTK часто сводится к классификации текста на позитивный или негативный.  \n",
    "Чтобы реализовать анализ настроений, можно использовать разные подходы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простая классификация с использованием предварительно обученных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "text = \"NLTK is amazing for natural language processing!\"\n",
    "print(sia.polarity_scores(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классификация с использованием настраиваемых тренировочных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.nltk.org/howto/sentiment.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('subjectivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "\n",
    "n_instances = 100\n",
    "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
    "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n",
    "train_subj_docs = subj_docs[:80]\n",
    "test_subj_docs = subj_docs[80:100]\n",
    "train_obj_docs = obj_docs[:80]\n",
    "test_obj_docs = obj_docs[80:100]\n",
    "training_docs = train_subj_docs+train_obj_docs\n",
    "testing_docs = test_subj_docs+test_obj_docs\n",
    "\n",
    "sentim_analyzer = SentimentAnalyzer()\n",
    "all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in training_docs])\n",
    "unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n",
    "sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)\n",
    "training_set = sentim_analyzer.apply_features(training_docs)\n",
    "test_set = sentim_analyzer.apply_features(testing_docs)\n",
    "\n",
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentim_analyzer.train(trainer, training_set)\n",
    "\n",
    "for key,value in sorted(sentim_analyzer.evaluate(test_set).items()):\n",
    "    print('{0}: {1}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью Vader  \n",
    "VADER (Valence Aware Dictionary and sEntiment Reasoner) — это модуль NLTK, который предоставляет оценки эмоций на основе словаря,  \n",
    "который сопоставляет лексические особенности с интенсивностью эмоций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentences = [\"VADER is smart, handsome, and funny.\", # positive sentence example\n",
    "    \"VADER is smart, handsome, and funny!\", # punctuation emphasis handled correctly (sentiment intensity adjusted)\n",
    "    \"VADER is very smart, handsome, and funny.\",  # booster words handled correctly (sentiment intensity adjusted)\n",
    "    \"VADER is VERY SMART, handsome, and FUNNY.\",  # emphasis for ALLCAPS handled\n",
    "    \"VADER is VERY SMART, handsome, and FUNNY!!!\",# combination of signals - VADER appropriately adjusts intensity\n",
    "    \"VADER is VERY SMART, really handsome, and INCREDIBLY FUNNY!!!\",# booster words & punctuation make this close to ceiling for score\n",
    "    \"The book was good.\",         # positive sentence\n",
    "    \"The book was kind of good.\", # qualified positive sentence is handled correctly (intensity adjusted)\n",
    "    \"The plot was good, but the characters are uncompelling and the dialog is not great.\", # mixed negation sentence\n",
    "    \"A really bad, horrible book.\",       # negative sentence with booster words\n",
    "    \"At least it isn't a horrible book.\", # negated negative sentence with contraction\n",
    "    \":) and :D\",     # emoticons handled\n",
    "    \"\",              # an empty string is correctly handled\n",
    "    \"Today sux\",     #  negative slang handled\n",
    "    \"Today sux!\",    #  negative slang with punctuation emphasis handled\n",
    "    \"Today SUX!\",    #  negative slang with capitalization emphasis\n",
    "    \"Today kinda sux! But I'll get by, lol\" # mixed sentiment example with slang and constrastive conjunction \"but\"\n",
    "]\n",
    "paragraph = \"It was one of the worst movies I've seen, despite good reviews. \\\n",
    "Unbelievably bad acting!! Poor direction. VERY poor production. \\\n",
    "The movie was bad. Very bad movie. VERY bad movie. VERY BAD movie. VERY BAD movie!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "lines_list = tokenize.sent_tokenize(paragraph)\n",
    "sentences.extend(lines_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tricky_sentences = [\n",
    "   \"Most automated sentiment analysis tools are shit.\",\n",
    "   \"VADER sentiment analysis is the shit.\",\n",
    "   \"Sentiment analysis has never been good.\",\n",
    "   \"Sentiment analysis with VADER has never been this good.\",\n",
    "   \"Warren Beatty has never been so entertaining.\",\n",
    "   \"I won't say that the movie is astounding and I wouldn't claim that \\\n",
    "   the movie is too banal either.\",\n",
    "   \"I like to hate Michael Bay films, but I couldn't fault this one\",\n",
    "   \"I like to hate Michael Bay films, BUT I couldn't help but fault this one\",\n",
    "   \"It's one thing to watch an Uwe Boll film, but another thing entirely \\\n",
    "   to pay for it\",\n",
    "   \"The movie was too good\",\n",
    "   \"This movie was actually neither that funny, nor super witty.\",\n",
    "   \"This movie doesn't care about cleverness, wit or any other kind of \\\n",
    "   intelligent humor.\",\n",
    "   \"Those who find ugly meanings in beautiful things are corrupt without \\\n",
    "   being charming.\",\n",
    "   \"There are slow and repetitive parts, BUT it has just enough spice to \\\n",
    "   keep it interesting.\",\n",
    "   \"The script is not fantastic, but the acting is decent and the cinematography \\\n",
    "   is EXCELLENT!\",\n",
    "   \"Roger Dodger is one of the most compelling variations on this theme.\",\n",
    "   \"Roger Dodger is one of the least compelling variations on this theme.\",\n",
    "   \"Roger Dodger is at least compelling as a variation on the theme.\",\n",
    "   \"they fall in love with the product\",\n",
    "   \"but then it breaks\",\n",
    "   \"usually around the time the 90 day warranty expires\",\n",
    "   \"the twin towers collapsed today\",\n",
    "   \"However, Mr. Carter solemnly argues, his client carried out the kidnapping \\\n",
    "   under orders and in the ''least offensive way possible.''\"\n",
    "]\n",
    "sentences.extend(tricky_sentences)\n",
    "for sentence in sentences:\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    print(sentence)\n",
    "    ss = sid.polarity_scores(sentence)\n",
    "    for k in sorted(ss):\n",
    "        print('{0}: {1}, '.format(k, ss[k]), end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Анализ настроений с использованием токенизатора и списка стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "text = \"NLTK is not bad for learning NLP.\"\n",
    "filtered_text = ' '.join([word for word in word_tokenize(text) if not word in stop_words])\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "ss = sia.polarity_scores(filtered_text)\n",
    "for k in sorted(ss):\n",
    "    print('{0}: {1}, '.format(k, ss[k]), end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Комбинирование лемматизации и анализа настроений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "text = \"The movie was not good. The plot was terrible!\"\n",
    "lemmatized_text = ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(text)])\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "ss = sia.polarity_scores(lemmatized_text)\n",
    "for k in sorted(ss):\n",
    "    print('{0}: {1}, '.format(k, ss[k]), end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использование TextBlob для анализа настроений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"I love NLTK. It's incredibly helpful!\"\n",
    "blob = TextBlob(text)\n",
    "print(blob.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rulemma "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот лемматизатор написан полностью на Питоне"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install git+https://github.com/Koziev/rulemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход лемматизатор принимает результаты частеречного разбора, который выполняется отдельной библиотекой rupostagger.  \n",
    "В свою очередь частеречная разметка выполняется по результатам токенизации, которую можно выполнить с помощью rutokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install rutokenizer rupostagger rulemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rutokenizer\n",
    "import rupostagger\n",
    "import rulemma\n",
    "\n",
    "\n",
    "lemmatizer = rulemma.Lemmatizer()\n",
    "lemmatizer.load()\n",
    "\n",
    "tokenizer = rutokenizer.Tokenizer()\n",
    "tokenizer.load()\n",
    "\n",
    "tagger = rupostagger.RuPosTagger()\n",
    "tagger.load()\n",
    "\n",
    "sent = u'Мяукая, голодные кошки ловят жирненьких хрюнделей'\n",
    "tokens = tokenizer.tokenize(sent)\n",
    "tags = tagger.tag(tokens)\n",
    "lemmas = lemmatizer.lemmatize(tags)\n",
    "for word, tags, lemma, *_ in lemmas:\n",
    "\tprint(u'{:15}\\t{:15}\\t{}'.format(word, lemma, tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natasha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://habr.com/ru/articles/516098/  \n",
    "https://github.com/natasha/natasha  \n",
    "https://nbviewer.org/github/natasha/natasha/blob/master/docs.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотека Natasha имеет множество функций включая токенизацию, морфологический анализ,   \n",
    "лемматизацию, синтаксический анализ и извлечение именованных сущностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import, initialize modules, build Doc object.\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    \n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "\n",
    "    Doc\n",
    ")\n",
    "\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "\n",
    "names_extractor = NamesExtractor(morph_vocab)\n",
    "\n",
    "text = 'Посол Израиля на Украине Йоэль Лион признался, что пришел в шок, узнав о решении властей Львовской области объявить 2019 год годом лидера запрещенной в России Организации украинских националистов (ОУН) Степана Бандеры. Свое заявление он разместил в Twitter. «Я не могу понять, как прославление тех, кто непосредственно принимал участие в ужасных антисемитских преступлениях, помогает бороться с антисемитизмом и ксенофобией. Украина не должна забывать о преступлениях, совершенных против украинских евреев, и никоим образом не отмечать их через почитание их исполнителей», — написал дипломат. 11 декабря Львовский областной совет принял решение провозгласить 2019 год в регионе годом Степана Бандеры в связи с празднованием 110-летия со дня рождения лидера ОУН (Бандера родился 1 января 1909 года). В июле аналогичное решение принял Житомирский областной совет. В начале месяца с предложением к президенту страны Петру Порошенко вернуть Бандере звание Героя Украины обратились депутаты Верховной Рады. Парламентарии уверены, что признание Бандеры национальным героем поможет в борьбе с подрывной деятельностью против Украины в информационном поле, а также остановит «распространение мифов, созданных российской пропагандой». Степан Бандера (1909-1959) был одним из лидеров Организации украинских националистов, выступающей за создание независимого государства на территориях с украиноязычным населением. В 2010 году в период президентства Виктора Ющенко Бандера был посмертно признан Героем Украины, однако впоследствии это решение было отменено судом. '\n",
    "doc = Doc(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into tokens and sentencies. Defines tokens and sents properties of doc. Uses Razdel internally.\n",
    "doc.segment(segmenter)\n",
    "print(doc.tokens[:5])\n",
    "print(doc.sents[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизация и сегментация текста на предложения и токены в Natasha осуществляются с помощью встроенной библиотеки Razdel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from razdel import tokenize, sentenize\n",
    "text = \"Текст для анализа.\"\n",
    "tokens = list(tokenize(text))\n",
    "sents = list(sentenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Морфологический анализатор Natasha, основанный на модели Slovnet,   \n",
    "позволяет извлекать богатую морфологическую информацию о каждом токене, такую как часть речи, род, число и падеж:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import MorphVocab, Doc\n",
    "\n",
    "morph_vocab = MorphVocab()\n",
    "doc = Doc(text)\n",
    "doc.segment(segmenter)\n",
    "doc.tag_morph(morph_tagger)\n",
    "\n",
    "for token in doc.tokens:\n",
    "    print(token.text, token.pos, token.feats)\n",
    "    \n",
    "# Call morph.print() to visualize morphology markup.\n",
    "doc.sents[0].morph.print()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизация в Natasha производится на основе того же морфологического анализа и использует Pymorphy для приведения слов к их начальной форме:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc.tokens:\n",
    "    token.lemmatize(morph_vocab)\n",
    "    print(token.text, token.lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Синтаксический анализатор Natasha основан на модели Slovnet и позволяет строить деревья зависимостей для предложений.   \n",
    "Например, визуализации синтаксического анализа будет выглядеть так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.parse_syntax(syntax_parser)\n",
    "for sent in doc.sents:\n",
    "    sent.syntax.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER - Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract standart named entities: names, locations, organizations.     \n",
    "# Depends on segmentation step. Defines spans property of doc. Uses Slovnet NER model internally. \n",
    "\n",
    "# Call ner.print() to visualize NER markup. Uses Ipymarkup internally. \n",
    "doc.tag_ner(ner_tagger)\n",
    "print(doc.spans[:5])\n",
    "doc.ner.print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named entity normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every NER span apply normalization procedure. Depends on NER, morphology and syntax steps. Defines normal property of doc.spans.\n",
    "\n",
    "for span in doc.spans:\n",
    "    span.normalize(morph_vocab)\n",
    "print(doc.spans[:5])\n",
    "{_.text: _.normal for _ in doc.spans if _.text != _.normal}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named entity parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse PER named entities into firstname, surname and patronymic. \n",
    "# Depends on NER step. Defines fact property of doc.spans. Uses Yargy-parser internally.\n",
    "\n",
    "# Natasha also has built in extractors for dates, money, address.\n",
    "\n",
    "# Можно извлечь также все типы, которые есть в span.type,  \n",
    "# указав вместо PER другой\n",
    "\n",
    "for span in doc.spans:\n",
    "   if span.type == PER:\n",
    "       span.extract_fact(names_extractor)\n",
    "       \n",
    "print(doc.spans[:5])\n",
    "{_.normal: _.fact.as_dict for _ in doc.spans if _.type == PER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepPavlov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deeppavlov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классификация намерений   \n",
    "помогает понять, что хочет пользователь, вводя запрос в чат-бота. Можно использовать предобученную модель для классификации намерений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov import build_model, configs\n",
    "\n",
    "# загрузка модели классификации намерений\n",
    "intent_model = build_model(configs.classifiers.intents_snips, download=True)\n",
    "\n",
    "# классификация намерения в запросе\n",
    "intent_predictions = intent_model(['Book a flight from New York to San Francisco'])\n",
    "print(intent_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Извлечение именованных сущностей используют для определения и классификации значимых информационных элементов в тексте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model = build_model(configs.ner.ner_ontonotes_bert_mult, download=True)\n",
    "\n",
    "# извлечение сущностей из текста\n",
    "ner_results = ner_model(['John works at Disney in California'])\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель для ответов на вопросы позволяет системе находить ответы на вопросы, указанные в контексте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_model = build_model(configs.squad.squad_bert, download=True)\n",
    "\n",
    "# получение ответа на вопрос из предоставленного контекста\n",
    "qa_result = qa_model([\"What is the capital of France?\", \"The capital of France is Paris.\"])\n",
    "print(qa_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MyStem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://yandex.ru/dev/mystem/doc/ru/  \n",
    "https://github.com/nlpub/pymystem3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MyStem — это продукт от Яндекса, предоставляющий возможности для морфологического и синтаксического анализа текстов.   \n",
    "Он работает как консольное приложение и доступен для различных операционных систем (Windows, Linux, MacOS).   \n",
    "MyStem использует собственные алгоритмы для определения начальной формы слова и его грамматических характеристик\n",
    "\n",
    "Например, базовая лемматизация и морфологический анализ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymystem3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вместо `m.lemmatize(x)` можно использовать `m.analyze(x)` с параметром `lemmatize=True`,  \n",
    "чтобы получить только леммы, а не полный морфологический анализ. Это может быть быстрее для лемматизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "m = Mystem()  # Создаем объект Mystem один раз\n",
    "\n",
    "def lemmatize_text(text):\n",
    "  \"\"\"Лемматизация текста.\"\"\"\n",
    "  return ''.join(m.analyze(text)[0]['analysis'][0]['lex'] for word in m.analyze(text))\n",
    "\n",
    "df['purpose_lem'] = df['purpose'].apply(lemmatize_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "m = Mystem()\n",
    "text = \"Мама мыла раму каждый вечер перед сном.\"\n",
    "lemmas = m.lemmatize(text)\n",
    "print(''.join(lemmas))\n",
    "\n",
    "# морфологический анализ\n",
    "analyzed = m.analyze(text)\n",
    "for word_info in analyzed:\n",
    "    if 'analysis' in word_info and word_info['analysis']:\n",
    "        gr = word_info['analysis'][0]['gr']\n",
    "        print(f\"{word_info['text']} - {gr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Частая задача для лемматизированных слов — подсчёт числа их упоминаний в тексте.  \n",
    "Для этого вызывают специальный контейнер Counter из модуля collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n', 'легкомысленный', ' ', 'речь', ' ']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from pymystem3 import Mystem\n",
    "m = Mystem() \n",
    "text = \"\"\"\n",
    "Легкомысленные речи за столом произносив, \n",
    "я сидел, раскинув плечи, неподвижен и красив.\n",
    "\"\"\"\n",
    "lemmas = m.lemmatize(text)\n",
    "lemmas[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'\\n': 2,\n",
       "         'легкомысленный': 1,\n",
       "         ' ': 8,\n",
       "         'речь': 1,\n",
       "         'за': 1,\n",
       "         'стол': 1,\n",
       "         'произносить': 1,\n",
       "         ', \\n': 1,\n",
       "         'я': 1,\n",
       "         'сидеть': 1,\n",
       "         ', ': 2,\n",
       "         'раскидывать': 1,\n",
       "         'плечо': 1,\n",
       "         'неподвижный': 1,\n",
       "         'и': 1,\n",
       "         'красивый': 1,\n",
       "         '.': 1})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting grammatical information and lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "text = \"Красивая мама красиво мыла раму\"\n",
    "m = Mystem()\n",
    "lemmas = m.lemmatize(text)\n",
    "\n",
    "print (\"lemmas:\", ''.join(lemmas))\n",
    "print (\"full info:\", json.dumps(m.analyze(text), ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- С помощью лематизации мы можем сократить количество категорий.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# лемматизация\n",
    "\n",
    "unique_purpose = data['purpose'].unique()\n",
    "lemmas_list = []\n",
    "m = Mystem()\n",
    "for purpose in unique_purpose:\n",
    "    lemmas = ''.join(m.lemmatize(purpose)).strip()\n",
    "    lemmas_list.append(lemmas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделяются группы:\n",
    "- операции с автомобилем (ключевое слово - автомобиль)\n",
    "- операции с недвижимостью (ключевые слова: жилье, недвижимость)\n",
    "- проведение свадьбы (ключевое слово: свадьба)\n",
    "- получение образования (ключевое слово: образование)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для назначения категории цели\n",
    "\n",
    "def create_category_purpose(row):\n",
    "    lem_purpose = m.lemmatize(row['purpose'])\n",
    "    try:\n",
    "        if 'автомобиль' in lem_purpose:\n",
    "            return 'операции с автомобилем'\n",
    "        if ('жилье' in lem_purpose) or ('недвижимость' in lem_purpose ):\n",
    "            return 'операции с недвижимостью'\n",
    "        if 'свадьба' in lem_purpose:\n",
    "            return 'проведение свадьбы'\n",
    "        if 'образование' in lem_purpose:\n",
    "            return 'получение образования'\n",
    "    except:\n",
    "        return 'нет категории'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyMorphy3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyMorphy2 также предоставляет функции для морфологического анализа текстов, но в отличие от MyStem,   \n",
    "PyMorphy2 полностью открыта и развивается сообществом. Библиотека использует словари OpenCorpora для анализа и может работать с русским и украинским языком.   \n",
    "PyMorphy2 также предлагает API.\n",
    "\n",
    "Примеры использования:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pymorphy3\n",
    "\n",
    "\n",
    "# Определение части речи и форм слова:\n",
    "\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "word = 'стали'\n",
    "parsed_word = morph.parse(word)[0]\n",
    "print(f\"Нормальная форма: {parsed_word.normal_form}\")\n",
    "print(f\"Часть речи: {parsed_word.tag.POS}\")\n",
    "print(f\"Полное морфологическое описание: {parsed_word.tag}\")\n",
    "\n",
    "# Согласование слова с числом:\n",
    "\n",
    "word = 'книга'\n",
    "parsed_word = morph.parse(word)[0]\n",
    "plural = parsed_word.make_agree_with_number(5).word\n",
    "print(f\"Множественное число слова '{word}': {plural}\")\n",
    "\n",
    "# Работа с различными падежами:\n",
    "\n",
    "word = 'город'\n",
    "parsed_word = morph.parse(word)[0]\n",
    "for case in ['nomn', 'gent', 'datv', 'accs', 'ablt', 'loct']:\n",
    "    form = parsed_word.inflect({case}).word\n",
    "    print(f\"{case}: {form}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель \"Мешок слов\" (Bag of Words, BoW) является основным методом представления текстовых данных в обработке естественного языка (Natural Language Processing, NLP).  \n",
    "Она преобразует текст в числовой вектор, где каждое слово в тексте представляется количеством его появлений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В модели BoW текст (например, предложение или документ) представляется в виде \"мешка\" его слов, не учитывая грамматику и порядок слов, но сохраняя мультиплицивность.  \n",
    "Это преобразование текста в набор чисел позволяет использовать стандартные методы машинного обучения, которые работают на числовых данных.\n",
    "\n",
    "Каждое уникальное слово в тексте соответствует определенному индексу (или \"слоту\") в векторе. Если слово встречается в тексте,   \n",
    "то в соответствующем слоте вектора записывается количество его появлений. Например, текст \"яблоко банан яблоко\" превратится в вектор [2, 1],   \n",
    "если индекс 0 соответствует слову \"яблоко\", а индекс 1 — слову \"банан\".\n",
    "\n",
    "В анализе настроений модель BoW используется для преобразования текстовых данных в формат, пригодный для алгоритмов машинного обучения.  \n",
    "Так, текстовые данные (например, отзывы пользователей) преобразуются в числовые векторы, на которых можно обучать классификаторы для определения,  \n",
    "например, позитивного или негативного отношения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis  \n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn.naive_bayes import GaussianNB  \n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.corpus import movie_reviews\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример данных\n",
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lemma = nltk.WordNetLemmatizer()\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "# Загрузка данных\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка данных\n",
    "texts = [' '.join(doc) for doc, _ in documents]\n",
    "labels = [1 if category == 'pos' else 0 for _, category in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_text = []\n",
    "# Токенизация и лематизация\n",
    "for sentence in texts:\n",
    "    sentence = sentence.lower()\n",
    "    tokens = word_tokenize(sentence)\n",
    "    lem_tokens = [lemma.lemmatize(token) for token in tokens]\n",
    "    lem_filt_tokens = [token for token in lem_tokens if token not in stop_words and token not in punctuation]\n",
    "    prep_sentence = ' '.join(lem_filt_tokens)\n",
    "    prep_text.append(prep_sentence)\n",
    "   \n",
    "# Создание BoW модели\n",
    "vectorizer = CountVectorizer()\n",
    "bow = vectorizer.fit_transform(prep_text)\n",
    "\n",
    "# Разделение данных на обучающую и тестовую выборку\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow, labels, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8033333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.82      0.81       304\n",
      "           1       0.81      0.79      0.80       296\n",
      "\n",
      "    accuracy                           0.80       600\n",
      "   macro avg       0.80      0.80      0.80       600\n",
      "weighted avg       0.80      0.80      0.80       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Обучение классификатора\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Оценка классификатора\n",
    "predictions = classifier.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "\n",
    "print(classification_report(y_test, predictions, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://storage.googleapis.com/kagglesdsdata/datasets/2050/3494/SPAM%20text%20message%2020170820%20-%20Data.csv?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240812%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240812T040114Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=3ac09c07b4cd550f8b107f1351f7a7480dbbf1a0d2c5db52868271c4be609692eb0db5a88a5ea8d88b153e6f4f82270fb2314971b03a9465c378b30c061c13f153acd0bf1844b5969525540ea17ecb88cb787fc635eb02c9d68fd8522ac9a0668cc732993093bf2a3759146967a52afd9572dcf53c7e01a3956f8a4dfc409238e58ff387fa2fea8c78e864e00d0199095c60db91aa168c323aad27e20e54a3e5cd538da28856854f3ba67a078304dacac9d2536a1e17164bbffc1203817bb3d50601e4b4e250898e83c97b6fb250848ba0174cacbc952944cd946b5a7618affebfc308fa8b9b55721fa87cae44a7bd5954b5c1a4c4c58470447ab8aff9c40ffd')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category                                            Message\n",
       "0         0  Go until jurong point, crazy.. Available only ...\n",
       "1         0                      Ok lar... Joking wif u oni...\n",
       "2         1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3         0  U dun say so early hor... U c already then say...\n",
       "4         0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Category'] = df['Category'].apply(lambda x: 0 if x == 'ham' else 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем слова. На основе слов пишем для каждого текста словарь, где каждое слово - это ключ,   \n",
    "а значение ключа - это сколько раз встречается данное слова в данном тексте.\n",
    "\n",
    "- Удалим все символы, не являющимися латинскими буквами  \n",
    "- Заглавные буквы меняем на строчные  \n",
    "- Разделим текст на слова  \n",
    "- В каждом слове выделяем корень слова  \n",
    "- Создаем список всех слов  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nlp - natural language processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_data = df.iloc[0,1]\n",
    "nlp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'[^a-zA-Z]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузим необходимые данные punkt, которые используются для токенизации текста\n",
    "nltk.download('punkt')\n",
    "# WordNet is often used for semantic analysis and part-of-speech tagging\n",
    "nltk.download('wordnet')\n",
    "# загрузим стоп-слова\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat',\n",
       " 'ok lar joking wif u oni',\n",
       " 'free entry in a wkly comp to win fa cup final tkts st may text fa to to receive entry question std txt rate t c s apply over s',\n",
       " 'u dun say so early hor u c already then say',\n",
       " 'nah i don t think he goes to usf he lives around here though']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description_list = []\n",
    "for descr in df.Message:\n",
    "    # Удаление всех не латинских букв\n",
    "    descr = pattern.sub(' ', descr)\n",
    "    # Во всех словах заглавные буквы меняем на строчные\n",
    "    descr = descr.lower()\n",
    "    # Переводим текст в отдельные слова\n",
    "    descr = nltk.word_tokenize(descr)\n",
    "    words = [lemma.lemmatize(word) for word in descr]\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    descr = ' '.join(descr)\n",
    "    description_list.append(descr)\n",
    "description_list[:5]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем bag-of-words, для этого выбираем 3000 максимально встречаемых слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стоп-слова — это такие слова, которые присутствуют в любом языке и не привносят никакой полезной информации в предложение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Часто встречаемые слова: ['aah' 'aathi' 'abi' 'ability' 'abiola' 'abj' 'able' 'absolutly' 'abt'\n",
      " 'abta' 'aburo' 'ac' 'academic' 'acc' 'accept' 'access' 'accident'\n",
      " 'accidentally' 'accordingly' 'account' 'accounts' 'ache' 'acl' 'aco'\n",
      " 'acted' 'acting' 'action' 'activate' 'active' 'activities' 'actor'\n",
      " 'actual' 'actually' 'ad' 'adam' 'add' 'addamsfa' 'added' 'addicted'\n",
      " 'addie' 'address' 'admin' 'administrator' 'admirer' 'admit' 'adore'\n",
      " 'adoring' 'ads' 'adult' 'advance' 'adventure' 'advice' 'advise' 'ae'\n",
      " 'aeronautics' 'aeroplane' 'affair' 'affairs' 'affection' 'afraid' 'aft'\n",
      " 'afternoon' 'aftr' 'ag' 'agalla' 'age' 'ages' 'ago' 'ah' 'aha' 'ahead'\n",
      " 'ahmad' 'aight' 'ain' 'aint' 'air' 'airport' 'airtel' 'aiya' 'aiyah'\n",
      " 'aiyar' 'aiyo' 'aj' 'aka' 'al' 'alaipayuthe' 'album' 'alcohol' 'alert'\n",
      " 'alex' 'alfie' 'algarve' 'ali' 'alive' 'allah' 'allow' 'allowed'\n",
      " 'alright' 'alrite' 'alwys']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "max_features = 3000\n",
    "count_vectorizer = CountVectorizer(max_features = max_features, stop_words = \"english\")\n",
    "sparce_matrix = count_vectorizer.fit_transform(description_list).toarray()\n",
    "print(f\"Часто встречаемые слова: {count_vectorizer.get_feature_names_out()[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "max_features = 3000\n",
    "tfidf_count_vectorizer = TfidfVectorizer(max_features = max_features, stop_words = \"english\")\n",
    "tfidf_sparce_matrix = tfidf_count_vectorizer.fit_transform(description_list).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv.vocabulary_ in this instance is a dict, where the keys are the words (features) that you've found and the values are indices\n",
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем 10 самых встречающихся слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = count_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 6, 4, ..., 6, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_list = sparce_matrix.sum(axis=0)\n",
    "count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_appears_words = np.vstack([word_list, count_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['ur', 'just', 'gt', 'lt', 'ok', 'day', 'free', 'know', 'll',\n",
       "        'come'],\n",
       "       [391, 374, 318, 316, 293, 293, 288, 272, 270, 254]], dtype=object)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_appears_words[:, np.argsort(most_appears_words[1])[::-1]][:,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "готовим данные для модели "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.iloc[:,0].values\n",
    "x = sparce_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tfidf = tfidf_sparce_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делим данные на тренировочные и тестовые"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train_tfidf, x_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(x_tfidf,y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем наивный байесовский классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy of our model: 0.8681614349775785\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB()\n",
    "nb.fit(x_train,y_train)\n",
    "print(\"the accuracy of our model: {}\".format(nb.score(x_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.86      0.92       971\n",
      "           1       0.49      0.91      0.64       144\n",
      "\n",
      "    accuracy                           0.87      1115\n",
      "   macro avg       0.74      0.89      0.78      1115\n",
      "weighted avg       0.92      0.87      0.88      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, nb.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем логистическую регрессию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our accuracy is: 0.9811659192825112\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(max_iter = 200)\n",
    "lr.fit(x_train,y_train)\n",
    "print(\"our accuracy is: {}\".format(lr.score(x_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       956\n",
      "           1       0.99      0.87      0.93       159\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.99      0.94      0.96      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, lr.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our accuracy is: 0.9596412556053812\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(max_iter = 200)\n",
    "lr.fit(x_train_tfidf,y_train_tfidf)\n",
    "print(\"our accuracy is: {}\".format(lr.score(x_test_tfidf,y_test_tfidf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       958\n",
      "           1       0.97      0.74      0.84       157\n",
      "\n",
      "    accuracy                           0.96      1115\n",
      "   macro avg       0.96      0.87      0.91      1115\n",
      "weighted avg       0.96      0.96      0.96      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_tfidf, lr.predict(x_test_tfidf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классификатор по методу ближайшего соседа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With KNN (K=3) accuracy is:  0.9390134529147982\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(x_train,y_train)\n",
    "#print('Prediction: {}'.format(prediction))\n",
    "print('With KNN (K=3) accuracy is: ',knn.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97       971\n",
      "           1       0.97      0.54      0.70       144\n",
      "\n",
      "    accuracy                           0.94      1115\n",
      "   macro avg       0.96      0.77      0.83      1115\n",
      "weighted avg       0.94      0.94      0.93      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, knn.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With random forest accuracy is:  0.9390134529147982\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "random_forest.fit(x_train,y_train)\n",
    "#print('Prediction: {}'.format(prediction))\n",
    "print('With random forest accuracy is: ',knn.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       971\n",
      "           1       0.98      0.85      0.91       144\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.98      0.93      0.95      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, random_forest.predict(x_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fbe58ca63fe33f9eeae9e71d10368d2b4a57f2b1b395836210cc60d362c66949"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
